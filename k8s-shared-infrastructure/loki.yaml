apiVersion: apps/v1
kind: Deployment
metadata:
  name: loki-shared
  namespace: tas-shared
  labels:
    app: loki-shared
    component: logging
spec:
  replicas: 1
  selector:
    matchLabels:
      app: loki-shared
  template:
    metadata:
      labels:
        app: loki-shared
        component: logging
    spec:
      containers:
      - name: loki
        image: grafana/loki:3.4.0
        ports:
        - containerPort: 3100
          name: http
        - containerPort: 9096
          name: grpc
        args:
        - -config.file=/etc/loki/loki-config.yml
        - -target=all
        volumeMounts:
        - name: loki-config
          mountPath: /etc/loki
        - name: loki-storage
          mountPath: /loki
        resources:
          requests:
            memory: "1Gi"
            cpu: "500m"
          limits:
            memory: "2Gi"
            cpu: "1000m"
        livenessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 45
          periodSeconds: 10
          timeoutSeconds: 1
          failureThreshold: 3
        readinessProbe:
          httpGet:
            path: /ready
            port: 3100
          initialDelaySeconds: 10
          periodSeconds: 5
          timeoutSeconds: 1
          failureThreshold: 3
      volumes:
      - name: loki-config
        configMap:
          name: loki-shared-config
      - name: loki-storage
        persistentVolumeClaim:
          claimName: loki-shared-pvc

---
apiVersion: v1
kind: Service
metadata:
  name: loki-shared
  namespace: tas-shared
  labels:
    app: loki-shared
    component: logging
spec:
  type: ClusterIP
  ports:
  - port: 3100
    targetPort: 3100
    protocol: TCP
    name: http
  - port: 9096
    targetPort: 9096
    protocol: TCP
    name: grpc
  selector:
    app: loki-shared

---
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: loki-shared-pvc
  namespace: tas-shared
spec:
  accessModes:
    - ReadWriteOnce
  resources:
    requests:
      storage: 50Gi

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: loki-shared-config
  namespace: tas-shared
  labels:
    app: loki-shared
    component: logging
data:
  loki-config.yml: |
    auth_enabled: false

    server:
      http_listen_port: 3100
      grpc_listen_port: 9096

    common:
      instance_addr: 127.0.0.1
      path_prefix: /loki
      storage:
        s3:
          endpoint: minio-shared.tas-shared:9000
          bucketnames: loki-data
          access_key_id: minioadmin
          secret_access_key: minioadmin123
          s3forcepathstyle: true
          insecure: true
      replication_factor: 1
      ring:
        kvstore:
          store: inmemory

    query_range:
      results_cache:
        cache:
          embedded_cache:
            enabled: true
            max_size_mb: 100

    schema_config:
      configs:
        - from: 2020-10-24
          store: tsdb
          object_store: s3
          schema: v13
          index:
            prefix: index_
            period: 24h

    ruler:
      alertmanager_url: http://alertmanager-shared.tas-shared:9093

    limits_config:
      reject_old_samples: true
      reject_old_samples_max_age: 168h
      ingestion_rate_mb: 10
      ingestion_burst_size_mb: 20
      per_stream_rate_limit: 3MB
      per_stream_rate_limit_burst: 15MB
      retention_period: 720h

    compactor:
      working_directory: /loki/boltdb-shipper-compactor
      compaction_interval: 10m
      retention_enabled: true
      retention_delete_delay: 2h
      retention_delete_worker_count: 150
      delete_request_store: s3

    analytics:
      reporting_enabled: false

---
apiVersion: apps/v1
kind: DaemonSet
metadata:
  name: alloy-shared
  namespace: tas-shared
  labels:
    app: alloy-shared
    component: logging
spec:
  selector:
    matchLabels:
      app: alloy-shared
  template:
    metadata:
      labels:
        app: alloy-shared
        component: logging
    spec:
      serviceAccount: alloy-shared-sa
      containers:
      - name: alloy
        image: grafana/alloy:v1.5.0
        ports:
        - containerPort: 12345
          name: http
        args:
        - run
        - --server.http.listen-addr=0.0.0.0:12345
        - --stability.level=public-preview
        - /etc/alloy/alloy-config.alloy
        volumeMounts:
        - name: alloy-config
          mountPath: /etc/alloy
        - name: varlog
          mountPath: /var/log
          readOnly: true
        - name: varlibdockercontainers
          mountPath: /var/lib/docker/containers
          readOnly: true
        - name: etcmachine
          mountPath: /etc/machine-id
          readOnly: true
        env:
        - name: HOSTNAME
          valueFrom:
            fieldRef:
              fieldPath: spec.nodeName
        - name: LOKI_ENDPOINT
          value: "http://loki-shared.tas-shared:3100"
        resources:
          requests:
            memory: "256Mi"
            cpu: "100m"
          limits:
            memory: "512Mi"
            cpu: "500m"
        livenessProbe:
          httpGet:
            path: /-/healthy
            port: 12345
          initialDelaySeconds: 10
          periodSeconds: 10
        readinessProbe:
          httpGet:
            path: /-/ready
            port: 12345
          initialDelaySeconds: 5
          periodSeconds: 5
      volumes:
      - name: alloy-config
        configMap:
          name: alloy-shared-config
      - name: varlog
        hostPath:
          path: /var/log
      - name: varlibdockercontainers
        hostPath:
          path: /var/lib/docker/containers
      - name: etcmachine
        hostPath:
          path: /etc/machine-id
          type: File
      tolerations:
      - key: node-role.kubernetes.io/master
        operator: Exists
        effect: NoSchedule
      - key: node-role.kubernetes.io/control-plane
        operator: Exists
        effect: NoSchedule

---
apiVersion: v1
kind: Service
metadata:
  name: alloy-shared
  namespace: tas-shared
  labels:
    app: alloy-shared
    component: logging
spec:
  type: ClusterIP
  ports:
  - port: 12345
    targetPort: 12345
    protocol: TCP
    name: http
  selector:
    app: alloy-shared

---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: alloy-shared-sa
  namespace: tas-shared

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: alloy-shared-role
rules:
- apiGroups: [""]
  resources: ["nodes", "services", "endpoints", "pods", "events"]
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources: ["configmaps"]
  verbs: ["get"]
- apiGroups: ["extensions", "apps"]
  resources: ["deployments", "replicasets", "daemonsets"]
  verbs: ["get", "list", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: alloy-shared-rolebinding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: alloy-shared-role
subjects:
- kind: ServiceAccount
  name: alloy-shared-sa
  namespace: tas-shared

---
apiVersion: v1
kind: ConfigMap
metadata:
  name: alloy-shared-config
  namespace: tas-shared
  labels:
    app: alloy-shared
    component: logging
data:
  alloy-config.alloy: |
    // Alloy configuration for TAS Kubernetes log collection
    logging {
      level = "debug"  // Changed to debug for troubleshooting
      format = "logfmt"
    }

    // Kubernetes API discovery
    discovery.kubernetes "pods" {
      role = "pod"
    }

    discovery.kubernetes "nodes" {
      role = "node"
    }

    // Relabel Kubernetes pods for log collection
    discovery.relabel "kubernetes_pods" {
      targets = discovery.kubernetes.pods.targets

      // Only keep pods in tas-shared namespace
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        regex = "tas-shared"
        action = "keep"
      }

      // Only keep running pods
      rule {
        source_labels = ["__meta_kubernetes_pod_phase"]
        regex = "Running"
        action = "keep"
      }

      // Set job label from pod name
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "job"
        regex = "(.+)-shared.*"
        replacement = "${1}"
      }

      // Set instance label
      rule {
        source_labels = ["__meta_kubernetes_pod_name"]
        target_label = "instance"
      }

      // Set namespace label
      rule {
        source_labels = ["__meta_kubernetes_namespace"]
        target_label = "namespace"
      }

      // Set node name
      rule {
        source_labels = ["__meta_kubernetes_node_name"]
        target_label = "node"
      }

      // Set container name
      rule {
        source_labels = ["__meta_kubernetes_pod_container_name"]
        target_label = "container"
      }

      // Set log path for containerd
      rule {
        source_labels = ["__meta_kubernetes_pod_uid", "__meta_kubernetes_pod_container_name"]
        target_label = "__path__"
        separator = "/"
        replacement = "/var/log/pods/*${1}/${2}/*.log"
      }
    }

    // Collect logs from Kubernetes pods
    loki.source.kubernetes "pods" {
      targets    = discovery.relabel.kubernetes_pods.output
      forward_to = [loki.process.kubernetes_logs.receiver]
    }

    // Process Kubernetes logs - FIXED: More lenient processing
    loki.process "kubernetes_logs" {
      forward_to = [loki.write.loki.receiver]

      // Parse container logs in CRI format (non-blocking)
      stage.cri {}

      // Extract log level (optional, won't drop if not found)
      stage.regex {
        expression = "(?i)(?P<detected_level>debug|info|warn|warning|error|fatal|panic)"
      }

      // Add detected level as label if found
      stage.labels {
        values = {
          detected_level = "",
        }
      }
    }

    // Send logs to Loki
    loki.write "loki" {
      endpoint {
        url = env("LOKI_ENDPOINT") + "/loki/api/v1/push"
        
        // Retry configuration
        max_backoff_period = "5m"
        max_backoff_retries = 10
        min_backoff_period = "500ms"
      }
      
      // External labels
      external_labels = {
        cluster = "tas-k8s",
        environment = "development",
      }
    }